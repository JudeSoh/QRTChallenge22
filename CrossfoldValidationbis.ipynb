{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d16483b2-3e6f-47bc-af4f-48f7609ac1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_10752\\1448834574.py:22: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only.\n",
      "  X_train_QRT_reshape = pd.concat([ X_train_QRT.T.shift(i+1).stack(dropna=False) for i in range(250) ], 1).dropna()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_10752\\1448834574.py:42: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only.\n",
      "  X_test_reshape = pd.concat([ X_test.T.shift(i+1).stack(dropna=False) for i in range(250) ], 1).dropna()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_10752\\1448834574.py:51: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only.\n",
      "  X_train_Q_reshape = pd.concat([ X_train_Q.T.shift(i+1).stack(dropna=False) for i in range(250) ], 1).dropna()\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.nn.functional import normalize\n",
    "import time\n",
    "import pickle\n",
    "from datetime import timedelta\n",
    "np.random.seed(6488)\n",
    "\n",
    "X_train_QRT = pd.read_csv('./X_train.csv', index_col=0, sep=',')\n",
    "X_train_QRT.columns.name = 'date'\n",
    "\n",
    "Y_train_QRT = pd.read_csv('./Y_train.csv', index_col=0, sep=',')\n",
    "Y_train_QRT.columns.name = 'date'\n",
    "\n",
    "X_train_QRT_reshape = pd.concat([ X_train_QRT.T.shift(i+1).stack(dropna=False) for i in range(250) ], 1).dropna()\n",
    "X_train_QRT_reshape.columns = pd.Index(range(1,251), name='timeLag')\n",
    "\n",
    "N0 = 3\n",
    "\n",
    "def split(j):\n",
    "            perm = np.random.permutation(range(50))\n",
    "            valset = perm[:j]\n",
    "            trainset = perm[j:]\n",
    "            X_1 = X_train_QRT.iloc[valset,]\n",
    "            X_2 = X_train_QRT.iloc[trainset,]\n",
    "            Y_1 = Y_train_QRT.iloc[valset,]\n",
    "            Y_2 = Y_train_QRT.iloc[trainset,]\n",
    "            return X_2,Y_2,X_1,Y_1,trainset\n",
    "\n",
    "X_train_Q,Y_train_Q,X_test,Y_test,test_locations = split(N0)\n",
    "        # Here we define our train and validation sets ! \n",
    "    \n",
    "X_test.columns.name = 'date' \n",
    "\n",
    "X_test_reshape = pd.concat([ X_test.T.shift(i+1).stack(dropna=False) for i in range(250) ], 1).dropna()\n",
    "X_test_reshape.columns = pd.Index(range(1,251), name='timeLag')\n",
    "\n",
    "#X_train_Q = pd.read_csv('./X_train.csv', index_col=0, sep=',')\n",
    "#X_train_Q.columns.name = 'date'\n",
    "\n",
    "#Y_train_Q = pd.read_csv('./Y_train.csv', index_col=0, sep=',')\n",
    "#Y_train_Q.columns.name = 'date'\n",
    "\n",
    "X_train_Q_reshape = pd.concat([ X_train_Q.T.shift(i+1).stack(dropna=False) for i in range(250) ], 1).dropna()\n",
    "X_train_Q_reshape.columns = pd.Index(range(1,251), name='timeLag')\n",
    "\n",
    "def randomA(D=250, F=10):  \n",
    "\n",
    "    M = np.random.randn(D,F)\n",
    "    randomStiefel = np.linalg.qr(M)[0] # Apply Gram-Schmidt algorithm to the columns of M\n",
    "\n",
    "    return randomStiefel\n",
    "\n",
    "def checkOrthonormality(A): \n",
    "\n",
    "        bool = True\n",
    "        D, F = A.shape   \n",
    "        Error = pd.DataFrame(A.T @ A - np.eye(F)).abs()\n",
    "\n",
    "        if any(Error.unstack() > 1e-6):\n",
    "            bool = False\n",
    "\n",
    "        return bool\n",
    "    \n",
    "def parametersTransform(A, beta, D=250, F=10):\n",
    "    \n",
    "    if A.shape != (D, F):\n",
    "        print('A has not the good shape')\n",
    "        return\n",
    "    \n",
    "    if beta.shape[0] != F:\n",
    "        print('beta has not the good shape')\n",
    "        return        \n",
    "    \n",
    "    output = np.hstack( (np.hstack([A.T, beta.reshape((F, 1))])).T )\n",
    "    \n",
    "    return output\n",
    "\n",
    "def fitBeta_QRT(A):\n",
    "\n",
    "            predictors = X_train_Q_reshape @ A # the dataframe of the 10 factors created from A with the (date, stock) in index\n",
    "            targets = Y_train_Q.T.stack()\n",
    "            beta = np.linalg.inv(predictors.T @ predictors) @ predictors.T @ targets\n",
    "\n",
    "            return beta.to_numpy()\n",
    "        \n",
    "def metric_QRT(A, beta): # Attention, on évalue la métrique sur X_train, l'échantillon sélectionné !! \n",
    "\n",
    "            if not checkOrthonormality(A):\n",
    "                return -1.0    \n",
    "\n",
    "            Ypred = (X_train_Q_reshape @ A @ beta).unstack().T    # remet les prédictions sous forme de matrice (50-n)*504. Les vecteurs de Ypred sont les prédictions !!      \n",
    "            Ytrue = Y_train_Q\n",
    "\n",
    "            Ytrue = Ytrue.div(np.sqrt((Ytrue**2).sum()), 1)    \n",
    "            Ypred = Ypred.div(np.sqrt((Ypred**2).sum()), 1)\n",
    "\n",
    "            meanOverlap = (Ytrue * Ypred).sum().mean()\n",
    "\n",
    "            return  meanOverlap  \n",
    "        \n",
    "def metric_test(A, beta): # On évalue la métrique sur X_test et Y_test définis au tout début, et auxquels le modèle n'a jamais eu accès.  \n",
    "\n",
    "            if not checkOrthonormality(A):\n",
    "                return -1.0    \n",
    "\n",
    "            Ypred = (X_test_reshape @ A @ beta).unstack().T    # remet les prédictions sous forme de matrice (50-n)*504. Les vecteurs de Ypred sont les prédictions !!      \n",
    "            Ytrue = Y_test\n",
    "\n",
    "            Ytrue = Ytrue.div(np.sqrt((Ytrue**2).sum()), 1)    \n",
    "            Ypred = Ypred.div(np.sqrt((Ypred**2).sum()), 1)\n",
    "\n",
    "            meanOverlap = (Ytrue * Ypred).sum().mean()\n",
    "\n",
    "            return  meanOverlap  \n",
    "        \n",
    "def metric_QRT_bch(A, beta): \n",
    "    \n",
    "    if not checkOrthonormality(A):\n",
    "        return -1.0    \n",
    "    \n",
    "    Ypred = (X_train_QRT_reshape @ A @ beta).unstack().T    # remet les prédictions sous forme de matrice 50*504. Les vecteurs de Ypred sont les prédictions !!      \n",
    "    Ytrue = Y_train_QRT\n",
    "    \n",
    "    Ytrue = Ytrue.div(np.sqrt((Ytrue**2).sum()), 1)    \n",
    "    Ypred = Ypred.div(np.sqrt((Ypred**2).sum()), 1)\n",
    "\n",
    "    meanOverlap = (Ytrue * Ypred).sum().mean()\n",
    "\n",
    "    return  meanOverlap  \n",
    "\n",
    "\n",
    "\n",
    "def Cross_FV(n=4,Niter = 100):\n",
    "    \n",
    "    max_accur = 0\n",
    "    \n",
    "    for l in range(Niter):\n",
    "        \n",
    "        \n",
    "    \n",
    "        def My_Own_CF(j):\n",
    "            perm = np.random.permutation(range(50-N0))\n",
    "            valset = perm[:j]\n",
    "            trainset = perm[j:]\n",
    "            X_1 = X_train_Q.iloc[valset,]\n",
    "            X_2 = X_train_Q.iloc[trainset,]\n",
    "            Y_1 = Y_train_Q.iloc[valset,]\n",
    "            Y_2 = Y_train_Q.iloc[trainset,]\n",
    "            return X_2,Y_2,X_1,Y_1,valset\n",
    "\n",
    "        X_train,Y_train,X_val,Y_val,val_locations = My_Own_CF(n)\n",
    "        # Here we define our train and validation sets ! \n",
    "\n",
    "\n",
    "        X_train_reshape = pd.concat([ X_train.T.shift(i+1).stack(dropna=False) for i in range(250) ], 1).dropna()\n",
    "        X_train_reshape.columns = pd.Index(range(1,251), name='timeLag')\n",
    "\n",
    "        X_val_reshape = pd.concat([ X_val.T.shift(i+1).stack(dropna=False) for i in range(250) ], 1).dropna()\n",
    "        X_val_reshape.columns = pd.Index(range(1,251), name='timeLag')\n",
    "\n",
    "        # Tout le bazar stdt, pas besoin de se le retaper à chaque fois\n",
    "        targets_t = torch.tensor(Y_train.T.stack().to_numpy().astype(np.float32)) # Le gros vecteur de pred \n",
    "        Y_true_t = torch.tensor(Y_train.to_numpy().astype(np.float32)) # La matrice à prédire en torch\n",
    "        targets_val_t = torch.tensor(Y_val.T.stack().to_numpy().astype(np.float32)) # Le gros vecteur de pred du validation set  \n",
    "        Y_true_val_t = torch.tensor(Y_val.to_numpy().astype(np.float32)) # La matrice à prédire du validation set en torch\n",
    "        X_train_reshape_t = torch.tensor(X_train_reshape.to_numpy().astype(np.float32)) # X_train_reshape en torch\n",
    "        X_val_reshape_t = torch.tensor(X_val_reshape.to_numpy().astype(np.float32)) # X_val_reshape en torch\n",
    "\n",
    "        def fitBeta_train(A):\n",
    "\n",
    "            predictors = X_train_reshape @ A # the dataframe of the 10 factors created from A with the (date, stock) in index\n",
    "            targets = Y_train.T.stack()\n",
    "            beta = np.linalg.inv(predictors.T @ predictors) @ predictors.T @ targets\n",
    "\n",
    "            return beta.to_numpy()\n",
    "        \n",
    "        def fitBeta_val(A): # A première vue on n'en a pas besoin, mais bon.  \n",
    "            \n",
    "            predictors = X_val_reshape @ A # the dataframe of the 10 factors created from A with the (date, stock) in index\n",
    "            targets = Y_val.T.stack()\n",
    "            beta = np.linalg.inv(predictors.T @ predictors) @ predictors.T @ targets\n",
    "\n",
    "            return beta.to_numpy()\n",
    "\n",
    "        def metric_train(A, beta): # Attention, on évalue la métrique sur X_train, l'échantillon sélectionné !! \n",
    "\n",
    "            if not checkOrthonormality(A):\n",
    "                return -1.0    \n",
    "\n",
    "            Ypred = (X_train_reshape @ A @ beta).unstack().T    # remet les prédictions sous forme de matrice (50-n)*504. Les vecteurs de Ypred sont les prédictions !!      \n",
    "            Ytrue = Y_train\n",
    "\n",
    "            Ytrue = Ytrue.div(np.sqrt((Ytrue**2).sum()), 1)    \n",
    "            Ypred = Ypred.div(np.sqrt((Ypred**2).sum()), 1)\n",
    "\n",
    "            meanOverlap = (Ytrue * Ypred).sum().mean()\n",
    "\n",
    "            return  meanOverlap  \n",
    "\n",
    "        def metric_val(A, beta): # On évalue la métrique sur le complémentaire \n",
    "\n",
    "            if not checkOrthonormality(A):\n",
    "                return -1.0    \n",
    "\n",
    "            Ypred = (X_val_reshape @ A @ beta).unstack().T    # remet les prédictions sous forme de matrice n*504. Les vecteurs de Ypred sont les prédictions du validation set!!      \n",
    "            Ytrue = Y_val\n",
    "\n",
    "            Ytrue = Ytrue.div(np.sqrt((Ytrue**2).sum()), 1)    \n",
    "            Ypred = Ypred.div(np.sqrt((Ypred**2).sum()), 1)\n",
    "\n",
    "            meanOverlap = (Ytrue * Ypred).sum().mean()\n",
    "\n",
    "            return  meanOverlap  \n",
    "\n",
    "        #On calcule en même temps la fonction à maximiser (sortie 1) et son gradient au point A (sortie 2) sur le training set !! Pour ne pas changer le bordel dans algo_curv_search (notations etc...), \n",
    "        # on définit cette fonction à l'intérieur de algo_curv_search ! \n",
    "        def F_et_gradient_train(A):\n",
    "\n",
    "            A_t = torch.tensor(A.astype(np.float32),requires_grad = True) # On transforme l'entrée en tensor (on spécifie qu'on va différencier par rapport à A_t !!)\n",
    "            predictors = X_train_reshape_t @ A_t # Les prédicteurs en tensor \n",
    "            beta_t = torch.inverse(predictors.T @ predictors)@predictors.T @targets_t # beta de l'équation normale en tensor\n",
    "            Y_pred_t = (X_train_reshape_t @ A_t @ beta_t).resize(504,(50-N0-n)).T #La matrice de prédiction en tensor\n",
    "            f_A = torch.mean(sum(normalize(Y_pred_t,dim=0)*normalize(Y_true_t,dim=0)))\n",
    "            f_A.backward() # On calcule le gradient\n",
    "            A_output = -1*(A_t.grad).numpy() # Et voici le gradient en A !! C'est magique tellement c'est facile... \n",
    "            func_val = -1*f_A.detach().numpy() #la fonction de coût à minimiser évaluée en A \n",
    "\n",
    "            return func_val, A_output\n",
    "\n",
    "        def algo_curv_search_train(X_start = randomA(),rho1 = 0.000001,rho2 = 0.5,eps = 0.001,j = 40): # la descente de gradient sur X_train\n",
    "            start = time.time()\n",
    "            X = X_start\n",
    "            i,I = 0,np.eye(250)\n",
    "            f,G = F_et_gradient_train(X) # f is f(X) et G the gradient of f at point X \n",
    "            tau_min = 0.0000001\n",
    "            tau_max = 10\n",
    "            #runn_scores = []\n",
    "\n",
    "            while  (np.linalg.norm(G) >= eps) and (i <= j):\n",
    "                i += 1\n",
    "                #if i % 5 == 0:\n",
    "                    #print(\"Nombre d'itérations:\",i,\"score actuel:\", -f,\"gradient actuel\", np.linalg.norm(G))\n",
    "                    #print(time.ctime(time.time() - start)) # to keep track of the evolution of the algorithm\n",
    "                tau_min = 0.000001\n",
    "                tau_max = 10\n",
    "                W = G @ X.T - X @G.T #the skew symmetric matrix of the article _ we try to stick to the notations !  Remember, we need to update these at the end of the Armijo Wolfe conditions.          \n",
    "                DF_0 = np.trace(G.T @ W @ X) #the directional derivative F'(Y(0)). \n",
    "                M_inv_min = np.linalg.inv(I + tau_min/2 * W) # Useful inverse matrices which we do not want to compute twice\n",
    "                M_inv_max = np.linalg.inv(I + tau_max/2 * W)\n",
    "                Y_t_min = M_inv_min @ (I - tau_min/2 * W) @ X\n",
    "                Y_t_max = M_inv_max @ (I - tau_max/2 * W) @ X\n",
    "                f_t_max,G_t_max = F_et_gradient_train(Y_t_max)\n",
    "                f_t_min,G_t_min = F_et_gradient_train(Y_t_min)\n",
    "                cond_min = (f_t_min <= f + rho1 * tau_min * DF_0) # AW cond at tau_min\n",
    "                cond_max = (.5 * np.trace(G_t_max.T @ M_inv_max @ W @ (X + Y_t_max)) >= rho2* DF_0) #AW cond at tau_max\n",
    "                while (not cond_min) or (not cond_max) :\n",
    "                    if not cond_min:\n",
    "                        #print(\"Boucle d'itération:\",i,\"là c'est cond_min:\", tau_min,tau_max) #pour check\n",
    "                        if tau_min < 0.01:\n",
    "                            tau_min = min(10*tau_min,tau_max - 0.0000000001)\n",
    "                        else :\n",
    "                            tau_min = min(1.05 * tau_min,tau_max - 0.000000001) \n",
    "                        #tau_min = .5*(tau_min + tau_max) # dichotomy is way too violent in this case... \n",
    "                        M_inv_min = np.linalg.inv(I + tau_min/2 * W)\n",
    "                        Y_t_min = M_inv_min @ (I - tau_min/2 * W) @ X\n",
    "                        f_t_min,G_t_min = F_et_gradient_train(Y_t_min)\n",
    "                        cond_min = (f_t_min <= f + rho1 * tau_min * DF_0) # AW cond at tau_min\n",
    "                    elif not cond_max:\n",
    "                        #print(\"Boucle d'itération:\",i,\"là c'est cond_max:\", tau_min,tau_max) #pour check\n",
    "                        tau_max = max(tau_min+ 0.000000001,min(.95*tau_max,.5*(tau_max+tau_min))) # just to ensure that we always have tau_max > tau_min\n",
    "                        # tau_max = .5*(tau_min + tau_max) same comment\n",
    "                        M_inv_max = np.linalg.inv(I + tau_max/2 * W)\n",
    "                        Y_t_max = M_inv_max @ (I - tau_max/2 * W) @ X\n",
    "                        f_t_max,G_t_max = F_et_gradient_train(Y_t_max)\n",
    "                        cond_max = (.5 * np.trace(G_t_max.T @ M_inv_max @ W @ (X + Y_t_max)) >= rho2* DF_0) #AW cond at tau_max\n",
    "\n",
    "                tau_f = .5*(tau_min + tau_max) # t_k satisfying both AW conditions\n",
    "                Y_t_f = np.linalg.inv(I + tau_f/2 * W) @ (I - tau_f/2 * W) @ X\n",
    "                f_t_f,G_t_f = F_et_gradient_train(Y_t_f)\n",
    "                #cond_f_1 = (f_t_f <= f + rho1 * tau_f * DF_0) #for control\n",
    "                #cond_f_2 = (.5 * np.trace(G_t_f.T @ np.linalg.inv(I + tau_f/2 * W) @ W @ (X + Y_t_f)) >= rho2* DF_0) #for control\n",
    "                #print(\"Boucle d'itération:\",i,\"un tau qui va\", tau_f, cond_f_1, cond_f_2 ) #for control \n",
    "                X = np.linalg.inv(I + tau_f/2 * W) @ (I - tau_f/2 * W) @ X\n",
    "                f,G = F_et_gradient_train(X)\n",
    "                #runn_scores.append(f)\n",
    "                #if npmean(runn_scores[::-20])\n",
    "\n",
    "            #print(\"Le score final est:\",-f)\n",
    "            return X,-f\n",
    "        \n",
    "        \n",
    "        def F_et_gradient_val(A):\n",
    "\n",
    "            A_t = torch.tensor(A.astype(np.float32),requires_grad = True) # On transforme l'entrée en tensor (on spécifie qu'on va différencier par rapport à A_t !!)\n",
    "            predictors = X_val_reshape_t @ A_t # Les prédicteurs en tensor \n",
    "            beta_t = torch.inverse(predictors.T @ predictors)@predictors.T @targets_val_t # beta de l'équation normale en tensor\n",
    "            Y_pred_t = (X_val_reshape_t @ A_t @ beta_t).resize(504,n).T #La matrice de prédiction en tensor\n",
    "            f_A = torch.mean(sum(normalize(Y_pred_t,dim=0)*normalize(Y_true_val_t,dim=0)))\n",
    "            f_A.backward() # On calcule le gradient\n",
    "            A_output = -1*(A_t.grad).numpy() # Et voici le gradient en A !! C'est magique tellement c'est facile... \n",
    "            func_val = -1*f_A.detach().numpy() #la fonction de coût à minimiser évaluée en A \n",
    "\n",
    "            return func_val, A_output\n",
    "\n",
    "        def algo_curv_search_val(X_start = randomA(),rho1 = 0.000001,rho2 = 0.5,eps = 0.001,j = 40): # la descente de gradient sur X_train\n",
    "            start = time.time()\n",
    "            X = X_start\n",
    "            i,I = 0,np.eye(250)\n",
    "            f,G = F_et_gradient_val(X) # f is f(X) et G the gradient of f at point X \n",
    "            tau_min = 0.0000001\n",
    "            tau_max = 10\n",
    "            #runn_scores = []\n",
    "\n",
    "            while  (np.linalg.norm(G) >= eps) and (i <= j):\n",
    "                i += 1\n",
    "                #if i % 5 == 0:\n",
    "                    #print(\"Nombre d'itérations:\",i,\"score actuel:\", -f,\"gradient actuel\", np.linalg.norm(G))\n",
    "                    #print(time.ctime(time.time() - start)) # to keep track of the evolution of the algorithm\n",
    "                tau_min = 0.000001\n",
    "                tau_max = 10\n",
    "                W = G @ X.T - X @G.T #the skew symmetric matrix of the article _ we try to stick to the notations !  Remember, we need to update these at the end of the Armijo Wolfe conditions.          \n",
    "                DF_0 = np.trace(G.T @ W @ X) #the directional derivative F'(Y(0)). \n",
    "                M_inv_min = np.linalg.inv(I + tau_min/2 * W) # Useful inverse matrices which we do not want to compute twice\n",
    "                M_inv_max = np.linalg.inv(I + tau_max/2 * W)\n",
    "                Y_t_min = M_inv_min @ (I - tau_min/2 * W) @ X\n",
    "                Y_t_max = M_inv_max @ (I - tau_max/2 * W) @ X\n",
    "                f_t_max,G_t_max = F_et_gradient_val(Y_t_max)\n",
    "                f_t_min,G_t_min = F_et_gradient_val(Y_t_min)\n",
    "                cond_min = (f_t_min <= f + rho1 * tau_min * DF_0) # AW cond at tau_min\n",
    "                cond_max = (.5 * np.trace(G_t_max.T @ M_inv_max @ W @ (X + Y_t_max)) >= rho2* DF_0) #AW cond at tau_max\n",
    "                while (not cond_min) or (not cond_max) :\n",
    "                    if not cond_min:\n",
    "                        #print(\"Boucle d'itération:\",i,\"là c'est cond_min:\", tau_min,tau_max) #pour check\n",
    "                        if tau_min < 0.01:\n",
    "                            tau_min = min(10*tau_min,tau_max - 0.0000000001)\n",
    "                        else :\n",
    "                            tau_min = min(1.05 * tau_min,tau_max - 0.000000001) \n",
    "                        #tau_min = .5*(tau_min + tau_max) # dichotomy is way too violent in this case... \n",
    "                        M_inv_min = np.linalg.inv(I + tau_min/2 * W)\n",
    "                        Y_t_min = M_inv_min @ (I - tau_min/2 * W) @ X\n",
    "                        f_t_min,G_t_min = F_et_gradient_val(Y_t_min)\n",
    "                        cond_min = (f_t_min <= f + rho1 * tau_min * DF_0) # AW cond at tau_min\n",
    "                    elif not cond_max:\n",
    "                        #print(\"Boucle d'itération:\",i,\"là c'est cond_max:\", tau_min,tau_max) #pour check\n",
    "                        tau_max = max(tau_min+ 0.000000001,min(.95*tau_max,.5*(tau_max+tau_min))) # just to ensure that we always have tau_max > tau_min\n",
    "                        # tau_max = .5*(tau_min + tau_max) same comment\n",
    "                        M_inv_max = np.linalg.inv(I + tau_max/2 * W)\n",
    "                        Y_t_max = M_inv_max @ (I - tau_max/2 * W) @ X\n",
    "                        f_t_max,G_t_max = F_et_gradient_val(Y_t_max)\n",
    "                        cond_max = (.5 * np.trace(G_t_max.T @ M_inv_max @ W @ (X + Y_t_max)) >= rho2* DF_0) #AW cond at tau_max\n",
    "\n",
    "                tau_f = .5*(tau_min + tau_max) # t_k satisfying both AW conditions\n",
    "                Y_t_f = np.linalg.inv(I + tau_f/2 * W) @ (I - tau_f/2 * W) @ X\n",
    "                f_t_f,G_t_f = F_et_gradient_val(Y_t_f)\n",
    "                #cond_f_1 = (f_t_f <= f + rho1 * tau_f * DF_0) #for control\n",
    "                #cond_f_2 = (.5 * np.trace(G_t_f.T @ np.linalg.inv(I + tau_f/2 * W) @ W @ (X + Y_t_f)) >= rho2* DF_0) #for control\n",
    "                #print(\"Boucle d'itération:\",i,\"un tau qui va\", tau_f, cond_f_1, cond_f_2 ) #for control \n",
    "                X = np.linalg.inv(I + tau_f/2 * W) @ (I - tau_f/2 * W) @ X\n",
    "                f,G = F_et_gradient_val(X)\n",
    "                #runn_scores.append(f)\n",
    "                #if npmean(runn_scores[::-20])\n",
    "\n",
    "            #print(\"Le score final est:\",-f)\n",
    "            return X,-f\n",
    "        \n",
    "        \n",
    "    \n",
    "        X,sc = algo_curv_search_train()\n",
    "        X_val,sc_val = algo_curv_search_val()\n",
    "        beta = fitBeta_train(X)\n",
    "        cur_score = metric_val(X,beta)\n",
    "        cur_accur = (cur_score/sc_val)*100\n",
    "        QRTmet = metric_QRT(X,beta) # métrique QRT sur X_train\n",
    "        testmet = metric_test(X,beta)\n",
    "        met_QRT = metric_QRT_bch(X,beta) #métrique QRT benchmark\n",
    "        \n",
    "        #if l % 2 == 0:\n",
    "        print( \"Pour l'itération\",l,\"le score vaut\",cur_accur,\"la métrique QRT vaut\",QRTmet, \"et la métrique sur l'ensemble de validation vaut\",testmet)\n",
    "\n",
    "        if cur_accur > 10 and testmet > .045 and met_QRT > 0.14:\n",
    "            print(\"Got a record:\", cur_accur, \"with QRT metric:\",QRTmet, \"at\",val_locations,\"for iteration\",l, \"for n = \",n,\"with validation metric\",testmet,\"and benchmark QRT metric:\", met_QRT )\n",
    "            results.append([X,beta,QRTmet,cur_accur,testmet,met_QRT])\n",
    "                \n",
    "            \n",
    "    # the last result is DA best !! \n",
    "    # This whole procedure is probably overfitting quite a lot, but what else... \n",
    "        \n",
    "    return results\n",
    "            \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05df533f-70c9-4a06-8647-48917016e5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_10752\\1448834574.py:163: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only.\n",
      "  X_train_reshape = pd.concat([ X_train.T.shift(i+1).stack(dropna=False) for i in range(250) ], 1).dropna()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_10752\\1448834574.py:166: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only.\n",
      "  X_val_reshape = pd.concat([ X_val.T.shift(i+1).stack(dropna=False) for i in range(250) ], 1).dropna()\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\_tensor.py:760: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pour l'itération 0 le score vaut 15.284914360251742 la métrique QRT vaut 0.14671648167553072 et la métrique sur l'ensemble de validation vaut 0.050883561028491016\n",
      "Pour l'itération 1 le score vaut 8.001055553881042 la métrique QRT vaut 0.15026715458173603 et la métrique sur l'ensemble de validation vaut 0.04630942246077914\n",
      "Pour l'itération 2 le score vaut 6.361497249832899 la métrique QRT vaut 0.14900725544313628 et la métrique sur l'ensemble de validation vaut 0.04149026586071141\n",
      "Pour l'itération 3 le score vaut 7.734723938398807 la métrique QRT vaut 0.15064977431581125 et la métrique sur l'ensemble de validation vaut 0.04789904262471444\n",
      "Pour l'itération 4 le score vaut 23.32681144147807 la métrique QRT vaut 0.13552779898066503 et la métrique sur l'ensemble de validation vaut 0.03964939962193662\n",
      "Pour l'itération 5 le score vaut 9.465531692748693 la métrique QRT vaut 0.15202513989736832 et la métrique sur l'ensemble de validation vaut 0.043924481632950436\n",
      "Pour l'itération 6 le score vaut 10.060195208422236 la métrique QRT vaut 0.15124290567902754 et la métrique sur l'ensemble de validation vaut 0.04373738616181956\n",
      "Pour l'itération 7 le score vaut 36.47215645322254 la métrique QRT vaut 0.15177388636514827 et la métrique sur l'ensemble de validation vaut 0.039385909529693335\n",
      "Pour l'itération 8 le score vaut 7.579571388782328 la métrique QRT vaut 0.1462924651682332 et la métrique sur l'ensemble de validation vaut 0.05639527017911778\n",
      "Pour l'itération 9 le score vaut 8.475630749997306 la métrique QRT vaut 0.14191368680315955 et la métrique sur l'ensemble de validation vaut 0.048302578088207045\n",
      "Pour l'itération 10 le score vaut 18.355238318689825 la métrique QRT vaut 0.14462195034868433 et la métrique sur l'ensemble de validation vaut 0.032719836088065744\n",
      "Pour l'itération 11 le score vaut 15.917331127200478 la métrique QRT vaut 0.14785187271801667 et la métrique sur l'ensemble de validation vaut 0.055804116231063225\n",
      "Pour l'itération 12 le score vaut 14.672265104570512 la métrique QRT vaut 0.1521994715066485 et la métrique sur l'ensemble de validation vaut 0.04501205539278293\n",
      "Got a record: 14.672265104570512 with QRT metric: 0.1521994715066485 at [12 35 11  7] for iteration 12 for n =  4 with validation metric 0.04501205539278293 and benchmark QRT metric: 0.14263480849143717\n",
      "Pour l'itération 13 le score vaut 9.181963170611322 la métrique QRT vaut 0.14574612157068387 et la métrique sur l'ensemble de validation vaut 0.06028769007953587\n",
      "Pour l'itération 14 le score vaut 10.37408672553844 la métrique QRT vaut 0.14749064015934538 et la métrique sur l'ensemble de validation vaut 0.05067328615616269\n",
      "Pour l'itération 15 le score vaut 13.706803552930873 la métrique QRT vaut 0.1483475302452606 et la métrique sur l'ensemble de validation vaut 0.050403197764207326\n",
      "Pour l'itération 16 le score vaut 20.467240038350724 la métrique QRT vaut 0.15073664656161168 et la métrique sur l'ensemble de validation vaut 0.030364003824260438\n",
      "Pour l'itération 17 le score vaut 23.193375788751762 la métrique QRT vaut 0.1503680964701869 et la métrique sur l'ensemble de validation vaut 0.058692348132444325\n",
      "Got a record: 23.193375788751762 with QRT metric: 0.1503680964701869 at [27 16 31 25] for iteration 17 for n =  4 with validation metric 0.058692348132444325 and benchmark QRT metric: 0.14196589235478854\n",
      "Pour l'itération 18 le score vaut 13.06959192216011 la métrique QRT vaut 0.15155671155543582 et la métrique sur l'ensemble de validation vaut 0.041747578971329274\n",
      "Pour l'itération 19 le score vaut 27.281294626855733 la métrique QRT vaut 0.1513727043304746 et la métrique sur l'ensemble de validation vaut 0.04332874857152438\n",
      "Pour l'itération 20 le score vaut 18.836283183144502 la métrique QRT vaut 0.14949951620411125 et la métrique sur l'ensemble de validation vaut 0.03883736694388188\n",
      "Pour l'itération 21 le score vaut 13.312846998444261 la métrique QRT vaut 0.14346776763738217 et la métrique sur l'ensemble de validation vaut 0.03769058572808293\n",
      "Pour l'itération 22 le score vaut 20.007111316956184 la métrique QRT vaut 0.149188024443517 et la métrique sur l'ensemble de validation vaut 0.03379882311419632\n",
      "Pour l'itération 23 le score vaut 2.9384466773243 la métrique QRT vaut 0.14159698999594456 et la métrique sur l'ensemble de validation vaut 0.04100228992708453\n",
      "Pour l'itération 24 le score vaut 8.749744016405572 la métrique QRT vaut 0.1516645324323054 et la métrique sur l'ensemble de validation vaut 0.04747432060764263\n",
      "Pour l'itération 25 le score vaut 17.16343647397171 la métrique QRT vaut 0.1445275038673936 et la métrique sur l'ensemble de validation vaut 0.03725622091870776\n",
      "Pour l'itération 26 le score vaut 14.876252378279068 la métrique QRT vaut 0.15099895197937965 et la métrique sur l'ensemble de validation vaut 0.03623849614177262\n",
      "Pour l'itération 27 le score vaut 17.08585696423326 la métrique QRT vaut 0.14593390794094482 et la métrique sur l'ensemble de validation vaut 0.052320739480139745\n",
      "Pour l'itération 28 le score vaut 12.815836477046478 la métrique QRT vaut 0.1512437794043794 et la métrique sur l'ensemble de validation vaut 0.048271241950938015\n",
      "Got a record: 12.815836477046478 with QRT metric: 0.1512437794043794 at [11 31 28 46] for iteration 28 for n =  4 with validation metric 0.048271241950938015 and benchmark QRT metric: 0.14228215662431012\n",
      "Pour l'itération 29 le score vaut 30.30674573755213 la métrique QRT vaut 0.15160736136508407 et la métrique sur l'ensemble de validation vaut 0.039053713645750825\n",
      "Pour l'itération 30 le score vaut 7.204190810230135 la métrique QRT vaut 0.1508595029758238 et la métrique sur l'ensemble de validation vaut 0.050001448776739524\n",
      "Pour l'itération 31 le score vaut 15.178006781806955 la métrique QRT vaut 0.14963158499483384 et la métrique sur l'ensemble de validation vaut 0.039830605869967166\n",
      "Pour l'itération 32 le score vaut 7.016841829030474 la métrique QRT vaut 0.14532821907764468 et la métrique sur l'ensemble de validation vaut 0.03954259634890815\n",
      "Pour l'itération 33 le score vaut 11.791956897520887 la métrique QRT vaut 0.14748778150797417 et la métrique sur l'ensemble de validation vaut 0.043774274661618595\n",
      "Pour l'itération 34 le score vaut 15.415290998356648 la métrique QRT vaut 0.15138926848273945 et la métrique sur l'ensemble de validation vaut 0.04630846139606107\n",
      "Got a record: 15.415290998356648 with QRT metric: 0.15138926848273945 at [ 0  2  6 28] for iteration 34 for n =  4 with validation metric 0.04630846139606107 and benchmark QRT metric: 0.14143657007484206\n",
      "Pour l'itération 35 le score vaut 8.66283387753606 la métrique QRT vaut 0.14761225687364254 et la métrique sur l'ensemble de validation vaut 0.046333842904563896\n",
      "Pour l'itération 36 le score vaut 21.96224595226675 la métrique QRT vaut 0.15145645021559231 et la métrique sur l'ensemble de validation vaut 0.040880160233215765\n",
      "Pour l'itération 37 le score vaut 18.803842824373024 la métrique QRT vaut 0.1446669974891574 et la métrique sur l'ensemble de validation vaut 0.04012993467440934\n",
      "Pour l'itération 38 le score vaut 20.240058473244822 la métrique QRT vaut 0.14471042079319518 et la métrique sur l'ensemble de validation vaut 0.053038642744704545\n",
      "Pour l'itération 39 le score vaut 5.537303167720164 la métrique QRT vaut 0.13647910716337425 et la métrique sur l'ensemble de validation vaut 0.04211910576900945\n",
      "Pour l'itération 40 le score vaut 7.586699334280735 la métrique QRT vaut 0.1516792694785497 et la métrique sur l'ensemble de validation vaut 0.04294945253800576\n",
      "Pour l'itération 41 le score vaut 27.072561798019805 la métrique QRT vaut 0.15148128547296436 et la métrique sur l'ensemble de validation vaut 0.05596258424538923\n",
      "Got a record: 27.072561798019805 with QRT metric: 0.15148128547296436 at [ 6 25 26 31] for iteration 41 for n =  4 with validation metric 0.05596258424538923 and benchmark QRT metric: 0.1429412227654804\n",
      "Pour l'itération 42 le score vaut 20.57574377997295 la métrique QRT vaut 0.1507654017414975 et la métrique sur l'ensemble de validation vaut 0.03988013813817064\n",
      "Pour l'itération 43 le score vaut 13.10524760291333 la métrique QRT vaut 0.15046319065673963 et la métrique sur l'ensemble de validation vaut 0.03804134330665804\n",
      "Pour l'itération 44 le score vaut 11.198073828836936 la métrique QRT vaut 0.14857742115240222 et la métrique sur l'ensemble de validation vaut 0.042231130415142586\n",
      "Pour l'itération 45 le score vaut 13.813859927511713 la métrique QRT vaut 0.14738125832670612 et la métrique sur l'ensemble de validation vaut 0.028240645013628322\n",
      "Pour l'itération 46 le score vaut 7.6078476780116 la métrique QRT vaut 0.14803542623200125 et la métrique sur l'ensemble de validation vaut 0.04498791824820714\n",
      "Pour l'itération 47 le score vaut 20.246670206305915 la métrique QRT vaut 0.13546103747445923 et la métrique sur l'ensemble de validation vaut 0.04198334125626311\n",
      "Pour l'itération 48 le score vaut 16.021612711590844 la métrique QRT vaut 0.1474381267222585 et la métrique sur l'ensemble de validation vaut 0.054674865095074736\n",
      "Pour l'itération 49 le score vaut 9.534653043478537 la métrique QRT vaut 0.15029852211765146 et la métrique sur l'ensemble de validation vaut 0.04844069190857027\n",
      "Pour l'itération 50 le score vaut 18.67387918154743 la métrique QRT vaut 0.1508032229095588 et la métrique sur l'ensemble de validation vaut 0.04503389945890601\n",
      "Got a record: 18.67387918154743 with QRT metric: 0.1508032229095588 at [32 42 34 27] for iteration 50 for n =  4 with validation metric 0.04503389945890601 and benchmark QRT metric: 0.14160009054559497\n",
      "Pour l'itération 51 le score vaut 19.220037271569197 la métrique QRT vaut 0.14784146017086777 et la métrique sur l'ensemble de validation vaut 0.031613728973237276\n",
      "Pour l'itération 52 le score vaut 17.75712488068875 la métrique QRT vaut 0.1426725252780449 et la métrique sur l'ensemble de validation vaut 0.03301857814068561\n",
      "Pour l'itération 53 le score vaut -2.465028064231463 la métrique QRT vaut 0.1488708152604037 et la métrique sur l'ensemble de validation vaut 0.05649225823306384\n",
      "Pour l'itération 54 le score vaut 5.712045620385884 la métrique QRT vaut 0.14848557482429872 et la métrique sur l'ensemble de validation vaut 0.050176605708955824\n",
      "Pour l'itération 55 le score vaut 26.538943814251915 la métrique QRT vaut 0.15204017246553114 et la métrique sur l'ensemble de validation vaut 0.04434536824700345\n",
      "Pour l'itération 56 le score vaut 11.774602740676212 la métrique QRT vaut 0.1466206755997041 et la métrique sur l'ensemble de validation vaut 0.052804904866249155\n",
      "Pour l'itération 57 le score vaut 20.25579375692022 la métrique QRT vaut 0.15081343434751376 et la métrique sur l'ensemble de validation vaut 0.041747107404191026\n",
      "Pour l'itération 58 le score vaut 20.314878591045822 la métrique QRT vaut 0.15161550700628554 et la métrique sur l'ensemble de validation vaut 0.04108521405017297\n",
      "Pour l'itération 59 le score vaut 20.06143507157824 la métrique QRT vaut 0.14222599200464064 et la métrique sur l'ensemble de validation vaut 0.05213564653818731\n",
      "Pour l'itération 60 le score vaut 24.129018606806323 la métrique QRT vaut 0.15035789196774266 et la métrique sur l'ensemble de validation vaut 0.04488398020264123\n",
      "Pour l'itération 61 le score vaut 15.031538071069539 la métrique QRT vaut 0.1462234560539377 et la métrique sur l'ensemble de validation vaut 0.055090755904366885\n",
      "Pour l'itération 62 le score vaut 20.312731065409274 la métrique QRT vaut 0.1466179076562742 et la métrique sur l'ensemble de validation vaut 0.05768989347498883\n",
      "Pour l'itération 63 le score vaut 20.551999277413614 la métrique QRT vaut 0.15027000703992652 et la métrique sur l'ensemble de validation vaut 0.04020979000632693\n",
      "Pour l'itération 64 le score vaut 10.903650935500009 la métrique QRT vaut 0.14917201545492992 et la métrique sur l'ensemble de validation vaut 0.031744358112982096\n",
      "Pour l'itération 65 le score vaut 26.26155307217215 la métrique QRT vaut 0.14263337412933472 et la métrique sur l'ensemble de validation vaut 0.03039222738319994\n",
      "Pour l'itération 66 le score vaut 36.20902909157422 la métrique QRT vaut 0.1518767008361349 et la métrique sur l'ensemble de validation vaut 0.05096157295935713\n",
      "Got a record: 36.20902909157422 with QRT metric: 0.1518767008361349 at [15  5  6 31] for iteration 66 for n =  4 with validation metric 0.05096157295935713 and benchmark QRT metric: 0.14218756251287748\n",
      "Pour l'itération 67 le score vaut 8.063022014172562 la métrique QRT vaut 0.14860564674236143 et la métrique sur l'ensemble de validation vaut 0.05089264772918759\n",
      "Pour l'itération 68 le score vaut 11.552400388545658 la métrique QRT vaut 0.14840693618186274 et la métrique sur l'ensemble de validation vaut 0.04292935121635989\n",
      "Pour l'itération 69 le score vaut 13.491267896583754 la métrique QRT vaut 0.14544415936084565 et la métrique sur l'ensemble de validation vaut 0.04483868203015294\n",
      "Pour l'itération 70 le score vaut 5.546780085946294 la métrique QRT vaut 0.14354801681204016 et la métrique sur l'ensemble de validation vaut 0.05265714385269118\n",
      "Pour l'itération 71 le score vaut 13.413517421272928 la métrique QRT vaut 0.15221004999869348 et la métrique sur l'ensemble de validation vaut 0.05061017709255325\n",
      "Got a record: 13.413517421272928 with QRT metric: 0.15221004999869348 at [ 6  0  7 35] for iteration 71 for n =  4 with validation metric 0.05061017709255325 and benchmark QRT metric: 0.14241672957453946\n",
      "Pour l'itération 72 le score vaut 27.17673709340156 la métrique QRT vaut 0.1491487111741542 et la métrique sur l'ensemble de validation vaut 0.038454299507018855\n",
      "Pour l'itération 73 le score vaut 18.861770508593374 la métrique QRT vaut 0.14807977484089704 et la métrique sur l'ensemble de validation vaut 0.05321054914568581\n",
      "Pour l'itération 74 le score vaut 14.205391826780167 la métrique QRT vaut 0.14567205137628952 et la métrique sur l'ensemble de validation vaut 0.032847981469293776\n",
      "Pour l'itération 75 le score vaut 8.764725386464743 la métrique QRT vaut 0.15138858311417352 et la métrique sur l'ensemble de validation vaut 0.03553571057415018\n",
      "Pour l'itération 76 le score vaut 12.936362425926411 la métrique QRT vaut 0.14422919411002183 et la métrique sur l'ensemble de validation vaut 0.06321666644932028\n",
      "Pour l'itération 77 le score vaut 6.047411025415031 la métrique QRT vaut 0.14307700270675494 et la métrique sur l'ensemble de validation vaut 0.034177371833038164\n",
      "Pour l'itération 78 le score vaut 15.788138495631706 la métrique QRT vaut 0.15034411905217246 et la métrique sur l'ensemble de validation vaut 0.05717549604076225\n",
      "Got a record: 15.788138495631706 with QRT metric: 0.15034411905217246 at [35 21 34 31] for iteration 78 for n =  4 with validation metric 0.05717549604076225 and benchmark QRT metric: 0.14172389592456172\n",
      "Pour l'itération 79 le score vaut 19.903811702804514 la métrique QRT vaut 0.1502886664445028 et la métrique sur l'ensemble de validation vaut 0.04620808179736975\n",
      "Got a record: 19.903811702804514 with QRT metric: 0.1502886664445028 at [46 31  5 27] for iteration 79 for n =  4 with validation metric 0.04620808179736975 and benchmark QRT metric: 0.14085721549743419\n",
      "Pour l'itération 80 le score vaut 9.95329652494292 la métrique QRT vaut 0.14813615690527968 et la métrique sur l'ensemble de validation vaut 0.03163800835282023\n",
      "Pour l'itération 81 le score vaut 23.9137303925963 la métrique QRT vaut 0.15111011114623082 et la métrique sur l'ensemble de validation vaut 0.03852250671431998\n",
      "Pour l'itération 82 le score vaut 9.004116070186347 la métrique QRT vaut 0.1498496564710211 et la métrique sur l'ensemble de validation vaut 0.045252155220806854\n",
      "Pour l'itération 83 le score vaut 27.00106082095704 la métrique QRT vaut 0.1509269343999453 et la métrique sur l'ensemble de validation vaut 0.04157735677012535\n",
      "Pour l'itération 84 le score vaut 10.035736125499026 la métrique QRT vaut 0.1463887881228117 et la métrique sur l'ensemble de validation vaut 0.043789669003254886\n",
      "Pour l'itération 85 le score vaut 16.59331875438163 la métrique QRT vaut 0.14738282831177843 et la métrique sur l'ensemble de validation vaut 0.03741880630924242\n",
      "Pour l'itération 86 le score vaut 6.1832141779034355 la métrique QRT vaut 0.14482461083207468 et la métrique sur l'ensemble de validation vaut 0.03791877395968314\n",
      "Pour l'itération 87 le score vaut 28.111578640723174 la métrique QRT vaut 0.14344218675310422 et la métrique sur l'ensemble de validation vaut 0.042364782931255895\n",
      "Pour l'itération 88 le score vaut 26.152132633111986 la métrique QRT vaut 0.14768746687917655 et la métrique sur l'ensemble de validation vaut 0.04993464591430672\n",
      "Pour l'itération 89 le score vaut 16.87719062950256 la métrique QRT vaut 0.15142117261871632 et la métrique sur l'ensemble de validation vaut 0.04715077366284399\n",
      "Got a record: 16.87719062950256 with QRT metric: 0.15142117261871632 at [ 8  0 16 40] for iteration 89 for n =  4 with validation metric 0.04715077366284399 and benchmark QRT metric: 0.14172488249713514\n",
      "Pour l'itération 90 le score vaut 14.395660046791939 la métrique QRT vaut 0.15016249664839074 et la métrique sur l'ensemble de validation vaut 0.044768348671656175\n",
      "Pour l'itération 91 le score vaut 5.911966307139928 la métrique QRT vaut 0.15097563023690283 et la métrique sur l'ensemble de validation vaut 0.03883349864252655\n",
      "Pour l'itération 92 le score vaut 3.9157173204009874 la métrique QRT vaut 0.14577274237425936 et la métrique sur l'ensemble de validation vaut 0.06450600431631201\n",
      "Pour l'itération 93 le score vaut 14.98024896229459 la métrique QRT vaut 0.15012971129656758 et la métrique sur l'ensemble de validation vaut 0.04498293928076952\n",
      "Pour l'itération 94 le score vaut 22.59102981550487 la métrique QRT vaut 0.15161817286315318 et la métrique sur l'ensemble de validation vaut 0.053037824682429964\n",
      "Got a record: 22.59102981550487 with QRT metric: 0.15161817286315318 at [42 25 31 30] for iteration 94 for n =  4 with validation metric 0.053037824682429964 and benchmark QRT metric: 0.14298139167815258\n",
      "Pour l'itération 95 le score vaut 35.85062343776757 la métrique QRT vaut 0.1514066133551264 et la métrique sur l'ensemble de validation vaut 0.04409969007418969\n",
      "Pour l'itération 96 le score vaut 19.611526773546785 la métrique QRT vaut 0.14925339894902337 et la métrique sur l'ensemble de validation vaut 0.04727311363113197\n",
      "Got a record: 19.611526773546785 with QRT metric: 0.14925339894902337 at [36 27  4 10] for iteration 96 for n =  4 with validation metric 0.04727311363113197 and benchmark QRT metric: 0.14013452834504928\n",
      "Pour l'itération 97 le score vaut 23.547040111001028 la métrique QRT vaut 0.1508116985991876 et la métrique sur l'ensemble de validation vaut 0.04724622765853484\n",
      "Got a record: 23.547040111001028 with QRT metric: 0.1508116985991876 at [ 1 31 15 10] for iteration 97 for n =  4 with validation metric 0.04724622765853484 and benchmark QRT metric: 0.14161693726030658\n",
      "Pour l'itération 98 le score vaut 32.656349785878184 la métrique QRT vaut 0.1508626537755857 et la métrique sur l'ensemble de validation vaut 0.041976301634285874\n",
      "Pour l'itération 99 le score vaut 19.295252010970508 la métrique QRT vaut 0.14659783706010676 et la métrique sur l'ensemble de validation vaut 0.043790679332890485\n"
     ]
    }
   ],
   "source": [
    "\n",
    "np.random.seed(432339)\n",
    "results = []\n",
    "Last_res = Cross_FV(n=4,Niter = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "00443e43-0ac8-4627-b165-5a850b986adb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le score courant est  0.058692348132444325  pour l'indice 1 et une benchQRT 0.058692348132444325\n",
      "Le score courant est  0.05596258424538923  pour l'indice 4 et une benchQRT 0.05596258424538923\n",
      "Le score courant est  0.05096157295935713  pour l'indice 6 et une benchQRT 0.05096157295935713\n",
      "Le score courant est  0.05061017709255325  pour l'indice 7 et une benchQRT 0.05061017709255325\n",
      "Le score courant est  0.05717549604076225  pour l'indice 8 et une benchQRT 0.05717549604076225\n",
      "Le score courant est  0.053037824682429964  pour l'indice 11 et une benchQRT 0.053037824682429964\n"
     ]
    }
   ],
   "source": [
    "for j in range(len(Last_res)):\n",
    "    if Last_res[j][4] > .05:\n",
    "        print (\"Le score courant est \",Last_res[j][4], \" pour l'indice\",j,\"et une benchQRT\",Last_res[j][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "58f5d5be-c2c0-4cc7-bf48-c9f29843c322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14298139167815258"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Last_res[11][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bcde1ba1-f0d6-4079-adb2-df603a2349f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le score sur la métrique QRT de l'élément 1 est 0.14845519508497604 et son score était 24.041381853519727\n",
      "Le score sur la métrique QRT de l'élément 2 est 0.1481737786630536 et son score était 25.07653179183571\n",
      "Le score sur la métrique QRT de l'élément 3 est 0.14059700450100274 et son score était 29.848205544378008\n",
      "Le score sur la métrique QRT de l'élément 4 est 0.14837132228630923 et son score était 34.54582577413859\n",
      "Le score sur la métrique QRT de l'élément 5 est 0.14852832823499354 et son score était 41.12205494200732\n",
      "Le score sur la métrique QRT de l'élément 6 est 0.14826523779211426 et son score était 44.46617267620694\n"
     ]
    }
   ],
   "source": [
    "for j in range(6):\n",
    "    mach = metric_QRT(results[j][0],results[j][1]) \n",
    "    print(\"Le score sur la métrique QRT de l'élément\",j+1,\"est\", mach,\"et son score était\",results[j][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b4f5feee-34e2-43b9-aac0-a591fc2fbd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "QRT19 = Last_res[11][0]\n",
    "BT19 = Last_res[11][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2a24c0a3-2776-421c-a014-55fc51324f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14298139167815258"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_QRT(QRT19,BT19)\n",
    "metric_QRT_bch(QRT19,BT19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f49e82c8-e36e-4c05-8d84-12d8e2fe2080",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# On met le résultat dans un fichier du bon nom. \n",
    "reslong_QRT = parametersTransform(QRT19,BT19)\n",
    "pd.DataFrame(reslong_QRT).to_csv('./exemple23_CVN0_3_n4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0220094b-ceff-4376-8f7f-7a7b1cdee3ab",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " # renvoie n top scorers pour Xtrain\n",
    "        def resultatsgrad(k = 5): \n",
    "            mat=[]\n",
    "            for i in range(k):\n",
    "                X = randomA()\n",
    "                E,sc = algo_curv_search(X_start=X)\n",
    "                mat.append(E)\n",
    "            return mat    \n",
    "\n",
    "\n",
    "        # On teste les n sur Xval; on renvoie la liste des scores sur X_val\n",
    "        def top_scorer(E):\n",
    "            res = []\n",
    "            i=1\n",
    "            for mat in E:\n",
    "                beta = fitBeta(mat)\n",
    "                score = metric_val(mat,beta)\n",
    "                print(\"Le score pour la matrice\",i,\"est\",score)\n",
    "                res.append(score)\n",
    "                i+=1\n",
    "            j = np.argmax(res)\n",
    "            print(\"Le top scorer est le no\",k)\n",
    "            return E[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "32def342-042c-4937-9633-34851a676969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b947bca7-37b6-455e-a380-5370eb002641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkOrthonormality(EX6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7ae29679-8750-4b6a-b3f9-bdd22802515d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "RES = []\n",
    "def multCV(n=16,Niter = 40):\n",
    "    the_res = []\n",
    "    for k in range(47-n,47):\n",
    "        curr_res = Cross_FV(k,Niter)\n",
    "        print(\"On en est à n=\",k,\"et le meilleur score sur le val set est\",curr_res[3])\n",
    "        RES.append(curr_res)\n",
    "        the_res.append(curr_res)\n",
    "        print(\" The best result for\",k,\"is\",curr_res[2])\n",
    "    return the_res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "03e9f0d9-8336-476c-83d5-4833113aa841",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_8048\\4282905070.py:24: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only.\n",
      "  X_train_reshape = pd.concat([ X_train.T.shift(i+1).stack(dropna=False) for i in range(250) ], 1).dropna()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_8048\\4282905070.py:27: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only.\n",
      "  X_val_reshape = pd.concat([ X_val.T.shift(i+1).stack(dropna=False) for i in range(250) ], 1).dropna()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got a record: 0.041017143933057075 at [48 25 20  9 46 21 24 45 29 30 14 11 38 27 13 16  0 15 39  8  5 17  3 36\n",
      " 33 42  4 37 35 41 28] for iteration 0 and for n =  31\n",
      "Got a record: 0.04185237837247378 at [40 37 46 39  2 18 38 34  4 48  6 41 14 20 22 16 43  1 12  0 13  9 28 35\n",
      " 31 24 47 49 25 32 15] for iteration 2 and for n =  31\n",
      "Got a record: 0.044029479196612896 at [13 19 14 49 43  9 21  4 20 31 40 38  5 45 18 24 28 42 30 47  8 23 11  7\n",
      "  0  3 48 12 37 25 26] for iteration 28 and for n =  31\n",
      "Got a record: 0.044903724849602415 at [33  6 14 18  4  3  1 46 21 40  9 15 48  5 20 30 23  8 16 25 31 10 47 44\n",
      " 17 11 22 36 41 32 19] for iteration 31 and for n =  31\n",
      "Got a record: 0.04491005717701575 at [47 19 22  7 31 42 35 39  3 18  0  5 12 16 36 43 17 45 40  6 34 10 23 44\n",
      "  9 33 20 28 11 25 29] for iteration 36 and for n =  31\n",
      "Got a record: 0.05376020512109236 at [22 45 10 17 48  1  4 20 35 29 34 16 43 40 13 28  0  9 49 41 44 39 36 21\n",
      " 27 46 31  3 15 38 33] for iteration 40 and for n =  31\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [102]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m7893\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m FINAL_RES_CV \u001b[38;5;241m=\u001b[39m \u001b[43mmultCV\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mNiter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [100]\u001b[0m, in \u001b[0;36mmultCV\u001b[1;34m(n, Niter)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m47\u001b[39m\u001b[38;5;241m-\u001b[39mn,\u001b[38;5;241m47\u001b[39m):\n\u001b[0;32m      5\u001b[0m     curr_res \u001b[38;5;241m=\u001b[39m Cross_FV(k,Niter)\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOn en est à n=\u001b[39m\u001b[38;5;124m\"\u001b[39m,k,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124met le meilleur score sur le val set est\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[43mcurr_res\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      7\u001b[0m     RES\u001b[38;5;241m.\u001b[39mappend(curr_res)\n\u001b[0;32m      8\u001b[0m     the_res\u001b[38;5;241m.\u001b[39mappend(curr_res)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "np.random.seed(7893)\n",
    "FINAL_RES_CV = multCV(n=16,Niter = 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f877ac-015c-42f5-ba97-6155ad6ad208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1cc12e64-dfc6-4209-9583-1a0381965dce",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle \n",
    "with open('Bch_22_11', 'wb') as f1:\n",
    "    pickle.dump(A, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7874d22a-76f1-47fb-9457-f41e5d660383",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "EX1 = res[0]\n",
    "Bt1 = res[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5b28b505-f044-4148-9fd4-6107c3415fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reslong_QRT6 = parametersTransform(EX6,Bt6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f0bba80a-c5cb-4358-8c64-4b81681d49a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(reslong_QRT6).to_csv('./exemple6_n=6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "68211f6c-6658-4233-a258-4f5e3f988274",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.02679517,  0.08662926,  0.03063641, ...,  0.0140525 ,\n",
       "          0.00048591, -0.00744233],\n",
       "        [-0.11514198, -0.01465818, -0.019173  , ...,  0.0049001 ,\n",
       "         -0.00443981, -0.06772024],\n",
       "        [-0.06543031, -0.02364359, -0.06694202, ..., -0.1071562 ,\n",
       "         -0.01239147,  0.04086212],\n",
       "        ...,\n",
       "        [-0.07298383,  0.0300135 ,  0.03702207, ...,  0.00301753,\n",
       "          0.05553628, -0.09928898],\n",
       "        [-0.04429377,  0.04963639,  0.00895816, ..., -0.05165773,\n",
       "         -0.08020385, -0.01180377],\n",
       "        [ 0.04241005,  0.04978732, -0.01228543, ...,  0.04607884,\n",
       "          0.06654275, -0.1203566 ]]),\n",
       " array([-0.00382212,  0.06816777, -0.00100298,  0.00198185,  0.08871158,\n",
       "         0.05637792,  0.02684837, -0.00907717, -0.02606879, -0.03190033]),\n",
       " 0.09863197768225744]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Last_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "81b88f6c-3d0b-403f-9e98-7e222c677078",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "XAV1 = Last_res[0]\n",
    "beta1 = Last_res[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "14a2d343-941a-4735-8253-565fa5cddbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_15976\\606951516.py:17: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only.\n",
      "  X_train_reshape = pd.concat([ X_train.T.shift(i+1).stack(dropna=False) for i in range(250) ], 1).dropna()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_15976\\606951516.py:20: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only.\n",
      "  X_val_reshape = pd.concat([ X_val.T.shift(i+1).stack(dropna=False) for i in range(250) ], 1).dropna()\n"
     ]
    }
   ],
   "source": [
    "        \n",
    "    n = 4\n",
    "    \n",
    "    def My_Own_CF(j):\n",
    "            perm = np.random.permutation(range(50))\n",
    "            valset = perm[:j]\n",
    "            trainset = perm[j:]\n",
    "            X_1 = X_train_Q.iloc[valset,]\n",
    "            X_2 = X_train_Q.iloc[trainset,]\n",
    "            Y_1 = Y_train_Q.iloc[valset,]\n",
    "            Y_2 = Y_train_Q.iloc[trainset,]\n",
    "            return X_2,Y_2,X_1,Y_1,valset\n",
    "\n",
    "    X_train,Y_train,X_val,Y_val,val_locations = My_Own_CF(n)\n",
    "    # Here we define our train and validation sets ! \n",
    "\n",
    "\n",
    "    X_train_reshape = pd.concat([ X_train.T.shift(i+1).stack(dropna=False) for i in range(250) ], 1).dropna()\n",
    "    X_train_reshape.columns = pd.Index(range(1,251), name='timeLag')\n",
    "\n",
    "    X_val_reshape = pd.concat([ X_val.T.shift(i+1).stack(dropna=False) for i in range(250) ], 1).dropna()\n",
    "    X_val_reshape.columns = pd.Index(range(1,251), name='timeLag')\n",
    "\n",
    "    # Tout le bazar stdt, pas besoin de se le retaper à chaque fois\n",
    "    targets_t = torch.tensor(Y_train.T.stack().to_numpy().astype(np.float32)) # Le gros vecteur de pred \n",
    "    Y_true_t = torch.tensor(Y_train.to_numpy().astype(np.float32)) # La matrice à prédire en torch\n",
    "    targets_val_t = torch.tensor(Y_val.T.stack().to_numpy().astype(np.float32)) # Le gros vecteur de pred du validation set  \n",
    "    Y_true_val_t = torch.tensor(Y_val.to_numpy().astype(np.float32)) # La matrice à prédire du validation set en torch\n",
    "    X_train_reshape_t = torch.tensor(X_train_reshape.to_numpy().astype(np.float32)) # X_train_reshape en torch\n",
    "    X_val_reshape_t = torch.tensor(X_val_reshape.to_numpy().astype(np.float32)) # X_val_reshape en torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f6bb28db-bc4b-4086-abeb-f05c5e824112",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def F_et_gradient_val(A):\n",
    "\n",
    "            A_t = torch.tensor(A.astype(np.float32),requires_grad = True) # On transforme l'entrée en tensor (on spécifie qu'on va différencier par rapport à A_t !!)\n",
    "            predictors = X_val_reshape_t @ A_t # Les prédicteurs en tensor \n",
    "            beta_t = torch.inverse(predictors.T @ predictors)@predictors.T @targets_val_t # beta de l'équation normale en tensor\n",
    "            Y_pred_t = (X_val_reshape_t @ A_t @ beta_t).resize(504,n).T #La matrice de prédiction en tensor\n",
    "            f_A = torch.mean(sum(normalize(Y_pred_t,dim=0)*normalize(Y_true_val_t,dim=0)))\n",
    "            f_A.backward() # On calcule le gradient\n",
    "            A_output = -1*(A_t.grad).numpy() # Et voici le gradient en A !! C'est magique tellement c'est facile... \n",
    "            func_val = -1*f_A.detach().numpy() #la fonction de coût à minimiser évaluée en A \n",
    "\n",
    "            return func_val, A_output\n",
    "\n",
    "    def algo_curv_search_val(X_start = randomA(),rho1 = 0.000001,rho2 = 0.5,eps = 0.001,j = 40): # la descente de gradient sur X_train\n",
    "            \n",
    "            start = time.time()\n",
    "            X = X_start\n",
    "            i,I = 0,np.eye(250)\n",
    "            f,G = F_et_gradient_val(X) # f is f(X) et G the gradient of f at point X \n",
    "            tau_min = 0.00000001\n",
    "            tau_max = 10\n",
    "            #runn_scores = []\n",
    "\n",
    "            while  (np.linalg.norm(G) >= eps) and (i <= j):\n",
    "                i += 1\n",
    "                if i % 5 == 0:\n",
    "                    print(\"Nombre d'itérations:\",i,\"score actuel:\", -f,\"gradient actuel\", np.linalg.norm(G))\n",
    "                    print(time.ctime(time.time() - start)) # to keep track of the evolution of the algorithm\n",
    "                tau_min = 0.0000001\n",
    "                tau_max = 10\n",
    "                W = G @ X.T - X @G.T #the skew symmetric matrix of the article _ we try to stick to the notations !  Remember, we need to update these at the end of the Armijo Wolfe conditions.          \n",
    "                DF_0 = np.trace(G.T @ W @ X) #the directional derivative F'(Y(0)). \n",
    "                M_inv_min = np.linalg.inv(I + tau_min/2 * W) # Useful inverse matrices which we do not want to compute twice\n",
    "                M_inv_max = np.linalg.inv(I + tau_max/2 * W)\n",
    "                Y_t_min = M_inv_min @ (I - tau_min/2 * W) @ X\n",
    "                Y_t_max = M_inv_max @ (I - tau_max/2 * W) @ X\n",
    "                f_t_max,G_t_max = F_et_gradient_val(Y_t_max)\n",
    "                f_t_min,G_t_min = F_et_gradient_val(Y_t_min)\n",
    "                cond_min = (f_t_min <= f + rho1 * tau_min * DF_0) # AW cond at tau_min\n",
    "                cond_max = (.5 * np.trace(G_t_max.T @ M_inv_max @ W @ (X + Y_t_max)) >= rho2* DF_0) #AW cond at tau_max\n",
    "                while (not cond_min) or (not cond_max) :\n",
    "                    if not cond_min:\n",
    "                        #print(\"Boucle d'itération:\",i,\"là c'est cond_min:\", tau_min,tau_max) #pour check\n",
    "                        if tau_min < 0.01:\n",
    "                            tau_min = min(10*tau_min,tau_max - 0.0000000001)\n",
    "                        else :\n",
    "                            tau_min = min(1.05 * tau_min,tau_max - 0.000000001) \n",
    "                        #tau_min = .5*(tau_min + tau_max) # dichotomy is way too violent in this case... \n",
    "                        M_inv_min = np.linalg.inv(I + tau_min/2 * W)\n",
    "                        Y_t_min = M_inv_min @ (I - tau_min/2 * W) @ X\n",
    "                        f_t_min,G_t_min = F_et_gradient_val(Y_t_min)\n",
    "                        cond_min = (f_t_min <= f + rho1 * tau_min * DF_0) # AW cond at tau_min\n",
    "                    elif not cond_max:\n",
    "                        #print(\"Boucle d'itération:\",i,\"là c'est cond_max:\", tau_min,tau_max) #pour check\n",
    "                        tau_max = max(tau_min+ 0.000000001,min(.95*tau_max,.5*(tau_max+tau_min))) # just to ensure that we always have tau_max > tau_min\n",
    "                        # tau_max = .5*(tau_min + tau_max) same comment\n",
    "                        M_inv_max = np.linalg.inv(I + tau_max/2 * W)\n",
    "                        Y_t_max = M_inv_max @ (I - tau_max/2 * W) @ X\n",
    "                        f_t_max,G_t_max = F_et_gradient_val(Y_t_max)\n",
    "                        cond_max = (.5 * np.trace(G_t_max.T @ M_inv_max @ W @ (X + Y_t_max)) >= rho2* DF_0) #AW cond at tau_max\n",
    "\n",
    "                tau_f = .5*(tau_min + tau_max) # t_k satisfying both AW conditions\n",
    "                Y_t_f = np.linalg.inv(I + tau_f/2 * W) @ (I - tau_f/2 * W) @ X\n",
    "                f_t_f,G_t_f = F_et_gradient_val(Y_t_f)\n",
    "                #cond_f_1 = (f_t_f <= f + rho1 * tau_f * DF_0) #for control\n",
    "                #cond_f_2 = (.5 * np.trace(G_t_f.T @ np.linalg.inv(I + tau_f/2 * W) @ W @ (X + Y_t_f)) >= rho2* DF_0) #for control\n",
    "                #print(\"Boucle d'itération:\",i,\"un tau qui va\", tau_f, cond_f_1, cond_f_2 ) #for control \n",
    "                X = np.linalg.inv(I + tau_f/2 * W) @ (I - tau_f/2 * W) @ X\n",
    "                f,G = F_et_gradient_val(X)\n",
    "                #runn_scores.append(f)\n",
    "                #if npmean(runn_scores[::-20])\n",
    "\n",
    "            print(\"Le score final est:\",-f)\n",
    "            return X,-f\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b3e43f84-f6a6-4fe8-9b9f-86172e959c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = randomA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a09070e5-5632-4060-b0aa-952d73963d4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.040650974959135056,\n",
       " array([[-5.83904679e-04, -2.01293500e-03,  1.37763540e-03, ...,\n",
       "         -1.67436630e-03, -2.17501214e-03, -4.29129024e-04],\n",
       "        [-5.27584972e-03, -6.57886593e-03,  4.60390141e-03, ...,\n",
       "         -9.54798772e-04, -4.68726084e-03, -4.45055449e-03],\n",
       "        [-1.01493555e-04,  2.01926799e-03, -1.36127695e-03, ...,\n",
       "          2.60156998e-03,  2.67600059e-03, -1.91569736e-04],\n",
       "        ...,\n",
       "        [ 5.42101823e-03,  1.07930135e-02, -7.45559018e-03, ...,\n",
       "          5.90529712e-03,  1.00153079e-02,  4.37387312e-03],\n",
       "        [ 1.06903473e-02,  6.58742152e-04, -7.66952522e-04, ...,\n",
       "         -1.35369692e-02, -6.83750538e-03,  9.64376330e-03],\n",
       "        [-1.16239302e-03,  1.38287753e-04, -5.84391237e-05, ...,\n",
       "          1.72821176e-03,  1.01405894e-03, -1.05896045e-03]], dtype=float32))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_et_gradient_val(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1c75f63a-41d8-41cc-84ea-1b28e296d1bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'itérations: 5 score actuel: 0.1940990686416626 gradient actuel 0.27995858\n",
      "Thu Jan  1 01:00:04 1970\n",
      "Nombre d'itérations: 10 score actuel: 0.2860577404499054 gradient actuel 0.21378195\n",
      "Thu Jan  1 01:00:09 1970\n",
      "Nombre d'itérations: 15 score actuel: 0.3244090676307678 gradient actuel 0.16680072\n",
      "Thu Jan  1 01:00:14 1970\n",
      "Nombre d'itérations: 20 score actuel: 0.33814141154289246 gradient actuel 0.1394944\n",
      "Thu Jan  1 01:00:20 1970\n",
      "Nombre d'itérations: 25 score actuel: 0.3450325131416321 gradient actuel 0.8247676\n",
      "Thu Jan  1 01:00:28 1970\n",
      "Nombre d'itérations: 30 score actuel: 0.34542131423950195 gradient actuel 0.31794584\n",
      "Thu Jan  1 01:00:39 1970\n",
      "Nombre d'itérations: 35 score actuel: 0.3456016182899475 gradient actuel 0.13448928\n",
      "Thu Jan  1 01:00:50 1970\n",
      "Nombre d'itérations: 40 score actuel: 0.34561094641685486 gradient actuel 0.5829864\n",
      "Thu Jan  1 01:01:00 1970\n",
      "Le score final est: 0.34637486934661865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-0.04761862,  0.09420199,  0.04286247, ..., -0.01264419,\n",
       "         -0.02231062,  0.0222154 ],\n",
       "        [ 0.02350539, -0.08008093,  0.04950161, ..., -0.0512574 ,\n",
       "          0.05862464,  0.00989032],\n",
       "        [-0.00478209, -0.03881939, -0.06256097, ...,  0.04758623,\n",
       "          0.11898216, -0.08200416],\n",
       "        ...,\n",
       "        [ 0.02603782, -0.12641521, -0.01932352, ...,  0.02866886,\n",
       "          0.00374781, -0.10437706],\n",
       "        [ 0.02609229,  0.09916785, -0.04414824, ..., -0.02250776,\n",
       "          0.03002496, -0.1047299 ],\n",
       "        [-0.01258311,  0.05283789,  0.06962118, ..., -0.06002436,\n",
       "         -0.13489481,  0.06619275]]),\n",
       " 0.34637486934661865)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo_curv_search_val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fc8ed1-72e3-4af4-ad20-6faf96f1f68a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
